.. _running:

Running YANK
************

Hardware
========

Running on GPUs
"""""""""""""""

YANK uses `OpenMM <http://openmm.org>`_ as its simulation engine, which runs fastest on modern GPUs using either the ``CUDA` or ``OpenCL`` platforms.
Modern GTX-class hardware, such as the `GTX-1080 <http://www.geforce.com/hardware/10series/geforce-gtx-1080>`_ or `GTX-TITAN-X <http://www.geforce.com/hardware/desktop-gpus/geforce-gtx-titan-x>`_, should work very well.
See `Supported hardware`_ for more information about supported and recommended hardware.

Running on the CPU
""""""""""""""""""

If you don't have a GPU, the OpenMM ``CPU`` platform will run in multithreaded mode by default.
While not as fast as `gromacs <http://www.gromacs.org>`_, running on the CPU can still let you explore the features of YANK without needing a GPU.
You can also use CPUs acting as ``OpenCL`` devices utilizing the `AMD OpenCL APP SDK <http://developer.amd.com/tools-and-sdks/opencl-zone/>`_ or the `Intel OpenCL SDK <https://software.intel.com/en-us/intel-opencl>`_, though this has not been extensively tested.

.. note:: If you need to select how many threads YANK should use while running in ``CPU`` mode, you can set the environment variable ``OPENMM_CPU_THREADS``. See the `OpenMM manual <http://docs.openmm.org/7.0.0/userguide/library.html?highlight=openmm_cpu_threads#cpu-platform>`_ for more information.

Parallelization
===============

YANK can operate in two different modes:

Serial mode
"""""""""""

In :ref:`serial mode <serial-mode>`, a single simulation is run at a time, either on the GPU or CPU.
For replica-exchange calculations, this means that each replica is run serially, and then exchanges between replicas are allowed to occur.
This is useful if you are testing YANK, or running a large number of free energy calculations where speed is not a major concern.

MPI mode
""""""""

In :ref:`MPI mode <mpi-mode>`, multiple simulations can be run at once, either on multiple GPUs or multiple CPUs, thoughrunning simulations on a mixture of platforms is not supported at this time.
We utilize the widely-supported `Message Passing Interface (MPI) standard <http://www.mcs.anl.gov/research/projects/mpi/standard.html>`_ for parallelization.
All simulations are run using the same OpenMM ``Platform`` choice (``CUDA``, ``OpenCL``, ``CPU``, or ``Reference``)

In MPI mode, each YANK process utilizes the default GPU.
On systems with a single GPU/node, no special GPU management is required, but on systems with multiple GPUs/node, you will need to do some masking to ensure that YANK only sees the GPU it is supposed to use.
On systems with NVIDIA GPUs, this can easily be done by ensuring the ``CUDA_VISIBLE_DEVICES`` is set such that only the allocated GPU is seen by that YANK process.
See `Running in MPI mode`_ for more information on the tools we have provided to facilitate this.

Simulations may be started in one mode and then can be resumed using another parallelization mode or OpenMM ``Platform``.
The NetCDF files generated by YANK are platform-portable and hardware agnostic, so they can be moved from system to system if you want to start a simulation on one system and resume it elsewhere.

Parallelization is "pleasantly parallel", where information is exchanged only every few seconds or more.
This does not require high-bandwidth interconnects such as `Infiniband <https://en.wikipedia.org/wiki/InfiniBand>`_; 10Gbit/s ethernet connections should work very well.

.. _getting-help

Getting help
============

To get a list of all command-like options, simply use the ``--help`` flag:

.. code-block:: bash

   $ yank --help

|

.. _serial-mode:

Running in serial mode
======================

To run the simulation in serial mode, simply use ``yank run``, specifying a store directory by ``--store=dirname``:

.. code-block:: bash

   $ yank script --yaml=yank.yaml

The optional ``--verbose`` flag will show additional output during execution.

.. _mpi-mode:

Running in MPI mode
===================

Alternatively, to run the simulation in MPI mode:

.. code-block:: none

   $ yank script --yaml=yank.yaml

On systems with multiple NVIDIA GPUs per node, it is necessary to perform masking using ``CUDA_VISIBLE_DEVICES``.

On systems using the conda-installed ``mpi4py`` package, the `MPICH2 hydra mpirun <https://wiki.mpich.org/mpich/index.php/Using_the_Hydra_Process_Manager>`_ will be automatically installed for you.
You can use the cluster utility script `build-mpirun-configfile.py <https://github.com/choderalab/clusterutils/blob/master/scripts/build-mpirun-configfile.py>`_ available in our `clusterutils <https://github.com/choderalab/clusterutils>`_ to generate an appropriate ``configfile``:

.. code-block:: none

  $ python build-mpirun-configfile.py yank script --yaml=yank.yaml
  $ mpirun -configfile configfile

|

Selecting a platform
====================

OpenMM supports running simulations on a number of platforms, though not all platforms are available on all hardware.
To see which platforms your current installation supports, you can query the list of available platforms with

.. code-block:: none

  $ yank platforms
  Available OpenMM platforms:
      0 Reference
      1 CUDA
      2 CPU
      3 OpenCL

You can either leave the choice of platform up to YANK---in which case it will choose the fastest available platform---or specify
the desired platform via the ``--platform`` argument to ``yank``.  For example, to force YANK to use the ``OpenCL`` platform:

.. code-block:: bash

   $ yank script --yaml=yank.yaml --platform=OpenCL

.. note:: The ``CPU`` platform will automatically use all available cores/hyperthreads in serial mode, but in MPI mode, will use a single thread to avoid causing problems in queue-regulated parallel systems.  To control the number of threads yourself, set the ``OPENMM_NUM_THREADS`` environment variable to the desired number of threads.
